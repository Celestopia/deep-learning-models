import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import time
import tqdm

class FitHistory:
    '''
    Fit history class to record the training history of a model.
    '''
    def __init__(self):
        self.num_epochs=0
        self.epoch_time=[]
        self.train_loss=[]
        self.train_metric=[]
        self.val_loss=[]
        self.val_metric=[]
        self.metadata=None # to store any additional metadata

    def update(self, epoch_time, train_loss, train_metric, val_loss, val_metric):
        '''
        Parameters:
        - epoch_time: list. The time of training each epoch.
        - train_loss: list. The loss of training each epoch.
        - train_metric: list. The metric of training each epoch.
        - val_loss: list. The loss of validation each epoch.
        - val_metric: list. The metric of validation each epoch.
        '''
        self.num_epochs+=len(epoch_time)
        self.epoch_time.extend(epoch_time)
        self.train_loss.extend(train_loss)
        self.train_metric.extend(train_metric)
        self.val_loss.extend(val_loss)
        self.val_metric.extend(val_metric)

    def plot(self, figsize=(8,4)):
        plt.figure(figsize=figsize)
        plt.subplot(1, 2, 1)
        plt.plot(self.train_loss, label='train_loss')
        plt.plot(self.val_loss, label='val_loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(self.train_metric, label='train_metric')
        plt.plot(self.val_metric, label='val_metric')
        plt.xlabel('Epochs')
        plt.ylabel('Metric')
        plt.suptitle("Training History")
        plt.legend()
        plt.tight_layout() # adjust subplot spacing to prevent overlap
        # plt.savefig(save_path, dpi=300, bbox_inches='tight') # save picture
        plt.show()
    
    def summary(self):
        print(f'Number of epochs:  {self.num_epochs}')
        print(f'Training time:     {np.sum(self.epoch_time):.4f}s')
        print(f'Training loss:     {self.train_loss[-1]:.4f}')
        print(f'Training metric:   {self.train_metric[-1]:.4f}')
        print(f'Validation loss:   {self.val_loss[-1]:.4f}')
        print(f'Validation metric: {self.val_metric[-1]:.4f}')



def train(MODEL, train_loader, val_loader, optimizer,
            loss_func=nn.MSELoss(),
            metric_func=nn.L1Loss(),
            num_epochs=10,
            device='cpu',
            verbose=1
            ):
    """
    The train function is used to train a model with given data and parameters.

    Parameters:
    - MODEL: nn.Module. The model to be trained.
    - train_loader: DataLoader. The DataLoader for training data.
    - val_loader: DataLoader. The DataLoader for validation data.
    - optimizer: Optimizer. The optimizer used to update the model's weights.
    - loss_func: nn.modules.loss. The loss function used to calculate the loss between the predicted and true values.
    - metric_func: nn.modules.loss. The metric function used to calculate the metric between the predicted and true values.
    - num_epochs: int. The number of epochs to train the model.
    - device: str. The device to run the model. Can be 'cpu' or 'cuda'.
    - verbose: int. The level of verbosity.

    Return: a 5-tuple consisting of:
    - epoch_time_list: list. The time of training each epoch.
    - train_loss_list: list. The loss of training each epoch.
    - train_metric_list: list. The metric of training each epoch.
    - val_loss_list: list. The loss of validation each epoch.
    - val_metric_list: list. The metric of validation each epoch.    

    Annotations are in both Chinese and English. The initial version is Chinese (mostly auto-generated by AI).
    I translate them into English, but the original version is not deleted.
    """
    if not hasattr(MODEL, 'label_len'):
        # If the model doesn't have `label_len` attribute, the decoder input is not needed
        # 如果模型不含有label_len属性，说明前向传播过程不需要解码器输入
        epoch_time_list=[]
        train_loss_list=[]
        train_metric_list=[]
        val_loss_list=[]
        val_metric_list=[]
        total_time=0.0 # Total traning time # 总训练时间

        for epoch in tqdm.tqdm(range(num_epochs)) if verbose==1 else range(num_epochs):
            t1=time.time() # Start time this epoch # 该轮开始时间
            train_loss, train_metric = 0.0, 0.0 # training loss and metric this epoch # 本轮的训练loss和metric
            val_loss, val_metric = 0.0, 0.0 # validation loss and metric this epoch # 本轮的验证loss和metric

            # train # 训练
            MODEL.train() # switch to train mode # 切换到训练模式
            for inputs, targets in train_loader: # Iterate over batches of training data # 分批次遍历训练集
                inputs, targets = inputs.to(device), targets.to(device) # Transfer data to GPU (if available) # 将数据转移到GPU（如果可用）
                optimizer.zero_grad() # Empty the gradient # 清空梯度
                outputs = MODEL(inputs) # Forward pass # 前向传播
                loss = loss_func(outputs, targets)
                metric = metric_func(outputs, targets)
                loss.backward() # Backpropagation # 反向传播
                optimizer.step() # Update weights # 更新权重
                train_loss+=loss.item()
                train_metric+=metric.item()

            # test # 验证
            MODEL.eval() # switch to test mode # 切换到验证模式
            with torch.no_grad(): # close gradient calculation # 关闭梯度计算
                for inputs, targets in val_loader: # Iterate over batches of validation data # 分批次遍历验证集
                    inputs, targets = inputs.to(device), targets.to(device) # Transfer data to GPU (if available) # 将数据转移到GPU（如果可用）
                    outputs = MODEL(inputs) # Forward pass # 前向传播
                    loss = loss_func(outputs, targets)
                    metric=metric_func(outputs, targets)
                    val_loss+=loss.item()
                    val_metric+=metric.item()

            # Compute average values of each metric # 计算各指标的平均值
            average_train_loss=train_loss/len(train_loader) # Average train loss this epoch # 本轮的平均训练loss
            average_train_metric=train_metric/len(train_loader) # Average train metric this epoch # 本轮的平均训练metric
            average_val_loss=val_loss/len(val_loader) # Average val loss this epoch # 本轮的平均验证loss
            average_val_metric=val_metric/len(val_loader) # Average val metric this epoch # 本轮的平均验证metric

            # Compute training time this epoch # 计算本轮训练用时
            t2=time.time() # End time this epoch #该轮结束时间
            total_time+=(t2-t1) # Total training time #累计训练时间

            # Record epoch information # 记录本轮各指标值
            epoch_time_list.append(t2-t1)
            train_loss_list.append(average_train_loss)
            train_metric_list.append(average_train_metric)
            val_loss_list.append(average_val_loss)
            val_metric_list.append(average_val_metric)

            # Output message # 输出过程信息
            if verbose==1:
                message =f'Epoch [{str(epoch + 1).center(4, " ")}/{num_epochs}], Time: {(t2-t1):.4f}s'
                message+=f', Loss: {average_train_loss:.4f}'
                message+=f', Metric: {average_train_metric:.4f}'
                message+=f', Val Loss: {average_val_loss:.4f}'
                message+=f', Val Metric: {average_val_metric:.4f}'
                print(message)
        print(f'Total Time: {total_time:.4f}s')

        return (epoch_time_list, train_loss_list, train_metric_list, val_loss_list, val_metric_list)

    elif hasattr(MODEL, 'label_len') and MODEL.label_len > 0:
        # If the model has `label_len` attribute, decoder input is needed, and label should be taken into consideration
        # 如果模型含有label_len属性，说明前向传播过程需要解码器输入，训练过程考虑label
        label_len=MODEL.label_len
        output_len=MODEL.output_len
        pred_len=output_len-label_len

        epoch_time_list=[]
        train_loss_list=[]
        train_metric_list=[]
        val_loss_list=[]
        val_metric_list=[]
        total_time=0.0 # 总训练时间

        for epoch in tqdm.tqdm(range(num_epochs)) if verbose==1 else range(num_epochs):
            t1 = time.time() # Start time this epoch # 该轮开始时间
            train_loss, train_metric = 0.0, 0.0 # training loss and metric this epoch # 本轮的训练loss和metric
            val_loss, val_metric = 0.0, 0.0 # validation loss and metric this epoch # 本轮的验证loss和metric

            # train # 训练
            MODEL.train() # switch to train mode # 切换到训练模式
            for inputs, targets in train_loader:
                inputs, targets = inputs.to(device), targets.to(device) # Iterate over batches of training data # 将数据转移到GPU（如果可用）
                optimizer.zero_grad() # Empty the gradient # 清空梯度
                dec_inp = torch.cat([
                                    targets[:, :label_len, :],
                                    torch.zeros_like(targets[:, -pred_len:, :]).float().to(device)
                                    ],
                                    dim=1
                                    ).float().to(device) # Use targets as decoder input in the first `label_len` time steps, and zeros for the rest
                outputs = MODEL(inputs, dec_inp)
                outputs = outputs[:, -pred_len:, :].to(device) # Only take the last `pred_len` time steps # 取待预测时间范围内的数据
                targets = targets[:, -pred_len:, :].to(device) # Only take the last `pred_len` time steps # 取待预测时间范围内的数据
                loss = loss_func(outputs, targets)
                metric = metric_func(outputs, targets)
                loss.backward() # Backpropagation # 反向传播
                optimizer.step() # Update weights # 更新权重
                train_loss+=loss.item()
                train_metric+=metric.item()
            
            # test # 验证
            MODEL.eval() # switch to test mode # 切换到验证模式
            with torch.no_grad(): # close gradient calculation # 关闭梯度计算
                for inputs, targets in val_loader: # Iterate over batches of validation data # 分批次遍历验证集
                    inputs, targets = inputs.to(device), targets.to(device) # Transfer data to GPU (if available) # 将数据转移到GPU（如果可用）
                    dec_inp = torch.cat([
                                        inputs[:, -label_len:, :],
                                        torch.zeros_like(targets[:, -pred_len:, :]).float().to(device)
                                        ],
                                        dim=1
                                        ).float().to(device)
                    outputs = MODEL(inputs, dec_inp) # Forward pass # 前向传播
                    outputs = outputs[:, -pred_len:, :].to(device) # Only take the last `pred_len` time steps # 取待预测时间范围内的数据
                    targets = targets[:, -pred_len:, :].to(device) # Only take the last `pred_len` time steps # 取待预测时间范围内的数据
                    loss = loss_func(outputs, targets)
                    metric = metric_func(outputs, targets)
                    val_loss+=loss.item()
                    val_metric+=metric.item()
            
            # Compute average values of each metric # 计算各指标的平均值
            average_train_loss=train_loss/len(train_loader)
            average_train_metric=train_metric/len(train_loader)
            average_val_loss=val_loss/len(val_loader)
            average_val_metric=val_metric/len(val_loader)

            # Compute training time this epoch # 计算本轮训练用时
            t2=time.time() # End time this epoch #该轮结束时间
            total_time+=(t2-t1) # Total training time #累计训练时间

            # Record epoch information # 记录本轮各指标值
            epoch_time_list.append(t2-t1)
            train_loss_list.append(average_train_loss)
            train_metric_list.append(average_train_metric)
            val_loss_list.append(average_val_loss)
            val_metric_list.append(average_val_metric)
            
            # Output message # 输出过程信息
            if verbose==1:
                message =f'Epoch [{str(epoch + 1).center(4, " ")}/{num_epochs}], Time: {(t2-t1):.4f}s'
                message+=f', Loss: {average_train_loss:.4f}'
                message+=f', Metric: {average_train_metric:.4f}'
                message+=f', Val Loss: {average_val_loss:.4f}'
                message+=f', Val Metric: {average_val_metric:.4f}'
                print(message)
        print(f'Total Time: {total_time:.4f}s')

        return (epoch_time_list, train_loss_list, train_metric_list, val_loss_list, val_metric_list)


def plot_predictions(MODEL, X_grouped, Y_grouped, var_names, mat_paths,
                    iii=6,
                    figsize=(16,12),
                    device='cpu'
                    ):
    '''
    Plot the predictions of the model on a given mat file.
    Parameters:
    - MODEL: torch.nn.Module, the trained model
    - X_grouped: list of (list of (input_length,len(var_names)) numpy array), the input data grouped by mat file
    - Y_grouped: list of (list of (output_length,len(var_names)) numpy array), the output data grouped by mat file
    - var_names: list of strings, the names of the variables
    - mat_paths: list of strings, the paths of the mat files
    - iii: int, the index of the mat file to be plotted
    - figsize: tuple of int, the size of the figure
    Return:
    - None
    '''

    X_to_predict=[] # The input data to be predicted # 作为输入的真实数据
    Y_to_predict=[] # The output data to be predicted # 待预测的真实数据

    for i in range(len(X_grouped[iii])):
        X_to_predict.append(X_grouped[iii][i])
    X_to_predict=torch.Tensor(np.array(X_to_predict)).to(device) # X_to_predict: torch.Tensor. Shape: (num_batches, input_len, input_channels)

    for i in range(len(Y_grouped[iii])):
        Y_to_predict.append(Y_grouped[iii][i])
    Y_to_predict=torch.Tensor(np.array(Y_to_predict)).to(device) # Y_to_predict: torch.Tensor. Shape: (num_batches, output_len, output_channels):

    if not hasattr(MODEL, 'label_len'):
        # If the model doesn't have `label_len` attribute, the decoder input is not needed
        # 如果模型不含有label_len属性，说明前向传播过程不需要解码器输入
        Y_predicted=MODEL(X_to_predict)
    elif hasattr(MODEL, 'label_len') and MODEL.label_len > 0:
        # If the model has `label_len` attribute, decoder input is needed, and label should be taken into consideration
        # 如果模型含有label_len属性，说明前向传播过程需要解码器输入，训练过程考虑label
        label_len=MODEL.label_len
        output_len=MODEL.output_len
        pred_len=output_len-label_len
        dec_inp = torch.cat([                Y_to_predict[:, :label_len:, :],
                            torch.zeros_like(Y_to_predict[:, -pred_len:, :]).float().to(device)
                            ],
                            dim=1
                            ).float().to(device)
        Y_to_predict=Y_to_predict[:, -pred_len:, :] # Only take the last `pred_len` time steps # 取待预测时间范围内的数据
        Y_predicted=MODEL(X_to_predict, dec_inp)
    output_channels=Y_predicted.size(2)
    Y_predicted_flatten=Y_predicted.contiguous().view(-1,output_channels).detach().cpu().numpy()
    Y_to_predict_flatten=Y_to_predict.contiguous().view(-1,output_channels).detach().cpu().numpy()
    loss=np.mean((Y_predicted_flatten-Y_to_predict_flatten)**2) # Calculate the prediction loss # 计算预测误差

    # Plot the predictions # 绘制预测曲线
    plt.figure(figsize=figsize) # figsize is specified in the function parameter
    import os
    plt.suptitle('Time Series Prediction on {}\n Loss: {:.4f}'.format(
        os.path.basename(mat_paths[iii]), # os.path.basename() returns the file name without the folder path
        loss)
        )
    for var_name in var_names:
        var_idx=var_names.index(var_name)
        plt.subplot(4, 4, var_idx+1)
        plt.plot(Y_predicted_flatten[:,var_idx], alpha=0.9, c='red')
        plt.plot(Y_to_predict_flatten[:,var_idx], alpha=0.6, c='blue')
        plt.legend(['predict', 'true'], loc='upper right')
        plt.title(var_name)
    plt.tight_layout(h_pad=2) # adjust subplot spacing to prevent overlap
    #plt.savefig("", bbox_inches='tight')
    plt.show()



